{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alboulayan_selfstudy10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoneerahAlboulayan/info5731/blob/master/Alboulayan_selfstudy10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "js0jyx5IblL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Chapter11"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bw9ZVmiXbo0t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_data(data, prob):\n",
        "  \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
        "  results = [], []\n",
        "  for row in data:\n",
        "    results[0 if random.random() < prob else 1].append(row)\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rs-E2iIUbo3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split(x, y, test_pct):\n",
        "  data = zip(x, y) \n",
        "  train, test = split_data(data, 1 - test_pct) \n",
        "  x_train, y_train = zip(*train) \n",
        "  x_test, y_test = zip(*test)\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3EVhiHejbo6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bbdef09-99c2-48c7-d404-b9d1f87bade0"
      },
      "cell_type": "code",
      "source": [
        "def accuracy(tp, fp, fn, tn):\n",
        "  correct = tp + tn\n",
        "  total = tp + fp + fn + tn\n",
        "  return correct / total\n",
        "\n",
        "print (accuracy(70, 4930, 13930, 981070))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.98114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jn4BP-w8bo8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fb54463-759e-49bc-c010-782247995653"
      },
      "cell_type": "code",
      "source": [
        "def precision(tp, fp, fn, tn):\n",
        "  return tp / (tp + fp)\n",
        "print (precision(70, 4930, 13930, 981070))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FoH7P3lmbo_N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0bf51ec-9806-4004-ba0c-a71314b69baf"
      },
      "cell_type": "code",
      "source": [
        "def recall(tp, fp, fn, tn):\n",
        "  return tp / (tp + fn)\n",
        "print (recall(70, 4930, 13930, 981070))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bjvFuDkNbpB4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def f1_score(tp, fp, fn, tn):\n",
        "  p = precision(tp, fp, fn, tn)\n",
        "  r = recall(tp, fp, fn, tn)\n",
        "  return 2 * p * r / (p + r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qVcwp1v3bpET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Chapter13"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAQID96ubpHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(message):\n",
        "  message = message.lower() \n",
        "  all_words = re.findall(\"[a-z0-9']+\", message) \n",
        "  return set(all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iyI1ldbCczXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_words(training_set):\n",
        "  \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
        "  counts = defaultdict(lambda: [0, 0])\n",
        "  for message, is_spam in training_set:\n",
        "    for word in tokenize(message):\n",
        "      counts[word][0 if is_spam else 1] += 1\n",
        "  return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bg0eGAELczZx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
        "  \"\"\"turn the word_counts into a list of triplets\n",
        "  w, p(w | spam) and p(w | ~spam)\"\"\"\n",
        "  return [(w,\n",
        "    (spam + k) / (total_spams + 2 * k),\n",
        "    (non_spam + k) / (total_non_spams + 2 * k))\n",
        "    for w, (spam, non_spam) in counts.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3DUl6LCczcl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def spam_probability(word_probs, message):\n",
        "  message_words = tokenize(message)\n",
        "  log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
        "  for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
        "    if word in message_words:\n",
        "      log_prob_if_spam += math.log(prob_if_spam)\n",
        "      log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
        "    else:\n",
        "      log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
        "      log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
        "  prob_if_spam = math.exp(log_prob_if_spam)\n",
        "  prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
        "  return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AV5UbmtOczfh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "  def __init__(self, k=0.5):\n",
        "    self.k = k\n",
        "    self.word_probs = []\n",
        "  def train(self, training_set):\n",
        "    num_spams = len([is_spam\n",
        "                     for message, is_spam in training_set\n",
        "                     if is_spam])\n",
        "    num_non_spams = len(training_set) - num_spams\n",
        "    word_counts = count_words(training_set)\n",
        "    self.word_probs = word_probabilities(word_counts,\n",
        "                                         num_spams,\n",
        "                                         num_non_spams,\n",
        "                                         self.k)\n",
        "  def classify(self, message):\n",
        "    return spam_probability(self.word_probs, message)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9SKZRGiqcziG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob, re\n",
        "path = r\"C:\\spam\\*\\*\"\n",
        "data = []\n",
        "for fn in glob.glob(path):\n",
        "  is_spam = \"ham\" not in fn\n",
        "  with open(fn,'r') as file:\n",
        "    for line in file:\n",
        "      if line.startswith(\"Subject:\"):\n",
        "        subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
        "        data.append((subject, is_spam))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oK0IjJd1dssL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "random.seed(0) \n",
        "train_data, test_data = split_data(data, 0.75)\n",
        "classifier = NaiveBayesClassifier()\n",
        "classifier.train(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWijWePNdsu1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classified = [(subject, is_spam, classifier.classify(subject))\n",
        "for subject, is_spam in test_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSsKaHxUdsxd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "counts = collections.Counter((is_spam, spam_probability > 0.5)\n",
        "for _, is_spam, spam_probability in classified)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDRFvf1Mds0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classified.sort(key=lambda row: row[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPsjwKOyfNaj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spammiest_hams = filter(lambda row: not row[1], classified)[-5:]\n",
        "hammiest_spams = filter(lambda row: row[1], classified)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3LNffk5fNdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def p_spam_given_word(word_prob):\n",
        "  \"\"\"uses bayes's theorem to compute p(spam | message contains word)\"\"\"\n",
        "  word, prob_if_spam, prob_if_not_spam = word_prob\n",
        "  return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
        "words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
        "spammiest_words = words[-5:]\n",
        "hammiest_words = words[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u3pqk7uPfNgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def drop_final_s(word):\n",
        "  return re.sub(\"s$\", \"\", word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_b3nqKVfNi0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Chapter17"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rnIRDLfkfNl_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def entropy(class_probabilities):\n",
        "  \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
        "  return sum(-p * math.log(p, 2)\n",
        "             for p in class_probabilities\n",
        "             if p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fqBofytFf_Km",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def class_probabilities(labels):\n",
        "  total_count = len(labels)\n",
        "  return [count / total_count\n",
        "          for count in collections.Counter(labels).values()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yRcGNN4wf_NW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_entropy(labeled_data):\n",
        "  labels = [label for _, label in labeled_data]\n",
        "  probabilities = class_probabilities(labels)\n",
        "  return entropy(probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3PR1ujQjf_QK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partition_entropy(subsets):\n",
        "  \"\"\"find the entropy from this partition of data into subsets\n",
        "  subsets is a list of lists of labeled data\"\"\"\n",
        "  total_count = sum(len(subset) for subset in subsets)\n",
        "  return sum( data_entropy(subset) * len(subset) / total_count\n",
        "    for subset in subsets )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BSsZhOXf_TH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputs = [\n",
        "({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'no'}, False),\n",
        "({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'yes'}, False),\n",
        "({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
        "({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
        "({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
        "({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, False),\n",
        "({'level':'Mid', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, True),\n",
        "({'level':'Senior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, False),\n",
        "({'level':'Senior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
        "({'level':'Junior', 'lang':'Python', 'tweets':'yes', 'phd':'no'}, True),\n",
        "({'level':'Senior', 'lang':'Python', 'tweets':'yes', 'phd':'yes'}, True),\n",
        "({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, True),\n",
        "({'level':'Mid', 'lang':'Java', 'tweets':'yes', 'phd':'no'}, True),\n",
        "({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, False)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vl53_7isf_Vz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partition_by(inputs, attribute):\n",
        "  \"\"\"each input is a pair (attribute_dict, label).\n",
        "  returns a dict : attribute_value -> inputs\"\"\"\n",
        "  groups = defaultdict(list)\n",
        "  for input in inputs:\n",
        "    key = input[0][attribute] # get the value of the specified attribute\n",
        "    groups[key].append(input) # then add this input to the correct list\n",
        "  return groups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fSoBQ96Jgb72",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partition_entropy_by(inputs, attribute):\n",
        "  \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
        "  partitions = partition_by(inputs, attribute)\n",
        "  return partition_entropy(partitions.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIhCFDrLgb-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "23f3e715-355b-491d-a44c-d706d0c4dc31"
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "\n",
        "for key in ['level','lang','tweets','phd']:\n",
        "  print (key, partition_entropy_by(inputs, key))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "level 0.6935361388961919\n",
            "lang 0.8601317128547441\n",
            "tweets 0.7884504573082896\n",
            "phd 0.8921589282623617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iKS6p1JbgcBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aef48091-18b2-47b5-f977-9ef509083125"
      },
      "cell_type": "code",
      "source": [
        "senior_inputs = [(input, label)\n",
        "                 for input, label in inputs if input[\"level\"] == \"Senior\"]\n",
        "for key in ['lang', 'tweets', 'phd']:\n",
        "  print (key, partition_entropy_by(senior_inputs, key))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lang 0.4\n",
            "tweets 0.0\n",
            "phd 0.9509775004326938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCd-bqOAgcEL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def classify(tree, input):\n",
        "  \"\"\"classify the input using the given decision tree\"\"\"\n",
        "  if tree in [True, False]:\n",
        "    return tree\n",
        "  attribute, subtree_dict = tree\n",
        "  subtree_key = input.get(attribute) \n",
        "  if subtree_key not in subtree_dict: \n",
        "    subtree_key = None \n",
        "  subtree = subtree_dict[subtree_key] \n",
        "  return classify(subtree, input) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pu9hzM6JhRau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_tree_id3(inputs, split_candidates=None):\n",
        "  if split_candidates is None:\n",
        "    split_candidates = inputs[0][0].keys()\n",
        "  num_inputs = len(inputs)\n",
        "  num_trues = len([label for item, label in inputs if label])\n",
        "  num_falses = num_inputs - num_trues\n",
        "  if num_trues == 0: return False \n",
        "  if num_falses == 0: return True \n",
        "  if not split_candidates: \n",
        "    return num_trues >= num_falses\n",
        "  best_attribute = min(split_candidates,\n",
        "                       key=functools.partial(partition_entropy_by, inputs))\n",
        "  partitions = partition_by(inputs, best_attribute)\n",
        "  new_candidates = [a for a in split_candidates\n",
        "                    if a != best_attribute]\n",
        "  subtrees = { attribute_value : build_tree_id3(subset, new_candidates)\n",
        "              for attribute_value, subset in partitions.iteritems() }\n",
        "  subtrees[None] = num_trues > num_falses\n",
        "  return (best_attribute, subtrees)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yZrntkA9hRdo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tree = build_tree_id3(inputs)\n",
        "classify(tree, { \"level\" : \"Junior\",\n",
        "                \"lang\" : \"Java\",\n",
        "                \"tweets\" : \"yes\",\n",
        "                \"phd\" : \"no\"} ) \n",
        "classify(tree, { \"level\" : \"Junior\",\n",
        "                \"lang\" : \"Java\",\n",
        "                \"tweets\" : \"yes\",\n",
        "                \"phd\" : \"yes\"} ) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tJ_C7WGPhRg4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classify(tree, { \"level\" : \"Intern\" } ) # True\n",
        "classify(tree, { \"level\" : \"Senior\" } ) # False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GshylXFlhRjj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forest_classify(trees, input):\n",
        "  votes = [classify(tree, input) for tree in trees]\n",
        "  vote_counts = Counter(votes)\n",
        "  return vote_counts.most_common(1)[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I8P3qjlgi1io",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if len(split_candidates) <= self.num_split_candidates:\n",
        "  sampled_split_candidates = split_candidates\n",
        "else:\n",
        "  sampled_split_candidates = random.sample(split_candidates,\n",
        "                                             self.num_split_candidates)\n",
        "best_attribute = min(sampled_split_candidates,\n",
        "                     key=partial(partition_entropy_by, inputs))\n",
        "partitions = partition_by(inputs, best_attribute)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}